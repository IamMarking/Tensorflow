{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN 卷积神经网络.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IamMarking/Tensorflow/blob/master/CNN_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-B14e9Of44D",
        "colab_type": "text"
      },
      "source": [
        "#### 传统神经网络存在的问题：\n",
        "- 权值太多，计算量太大，计算起来非常慢\n",
        "- 权值太多，需要大量样本进行计算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGJHwjr3BSo",
        "colab_type": "text"
      },
      "source": [
        "#### 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADnWceyoePBG",
        "colab_type": "code",
        "outputId": "40236134-11eb-41f2-89ce-b3f3d0af2db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "old_v = tf.logging.get_verbosity()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "tf.logging.set_verbosity(old_v)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMTVeGPn9yxV",
        "colab_type": "code",
        "outputId": "b0c4e50a-b092-49ab-ab13-7af1e7aab436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "batch_size = 100  # 1 次放入 100 张图片进行训练，每张图片维度是 784 维\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size  # 计算一共有多少个批次\n",
        "\n",
        "# 初始化权值\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape,stddev=0.1)  # 生成一个截断的正态分布\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 初始化偏置\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1,shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 卷积层\n",
        "def conv2d(x,W):\n",
        "  # x 输入的 tensor 的维度 [batch,in_height,in_width,in_channels]\n",
        "  # 卷积核的 shape [filter_height,filter_width,in_channels,out_channels]\n",
        "  # 步长第 0 个值和第 3 个值都是 1 ，1 代表 x 方向的步长，2 代表 y 方向的步长\n",
        "  # padding SAME 补零，VALID 不用补零\n",
        "  return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')  # 2 维的卷积操作，\n",
        "\n",
        "# 池化层\n",
        "def max_pool_2x2(x):\n",
        "  # ksize 窗口的大小，第 1 3 位置设置成 1，中间两个数代表池化窗口大小 2*2\n",
        "  # \n",
        "  return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
        "\n",
        "x = tf.placeholder(tf.float32,[None,784])   \n",
        "y = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "# 改变 x 的格式转为 4D 的向量 [batch,in_height,in_width,in_channels]\n",
        "x_image = tf.reshape(x,[-1,28,28,1])\n",
        "\n",
        "# 初始化第一个卷积层的权值和偏重\n",
        "W_conv1 = weight_variable([5,5,1,32])  # 5*5 的采样窗口，32 个卷积核从一个平面抽取特征\n",
        "b_conv1 = bias_variable([32])  # 每一个卷积核一个偏置项\n",
        "\n",
        "# 把 x_image 和权值向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1) # 进行 max-pooling \n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    batch_xs,batch_ys = mnist.train.next_batch(batch_size)  # 每 100 张依次进行训练\n",
        "    print(batch_xs)\n",
        "    print(batch_xs.shape)\n",
        "    sess.run(h_conv1,feed_dict={x:batch_xs,y:batch_ys})\n",
        "    print(x_image.shape)\n",
        "    print(h_conv1.shape)\n",
        "    print(h_pool1.shape)\n",
        "            "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(100, 784)\n",
            "(?, 28, 28, 1)\n",
            "(?, 28, 28, 32)\n",
            "(?, 14, 14, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8rs6bgbhPli",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "ff15285c-3282-47ea-e64c-7b7d545b7fe0"
      },
      "source": [
        "batch_size = 100  # 1 次放入 100 张图片进行训练，每张图片维度是 784 维\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size  # 计算一共有多少个批次\n",
        "\n",
        "# 初始化权值\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape,stddev=0.1)  # 生成一个截断的正态分布\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 初始化偏置\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1,shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 卷积层\n",
        "def conv2d(x,W):\n",
        "  # x 输入的 tensor 的维度 [batch,in_height,in_width,in_channels]\n",
        "  # 卷积核的 shape [filter_height,filter_width,in_channels,out_channels]\n",
        "  # 步长第 0 个值和第 3 个值都是 1 ，1 代表 x 方向的步长，2 代表 y 方向的步长\n",
        "  # padding SAME 补零，VALID 不用补零\n",
        "  return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')  # 2 维的卷积操作，\n",
        "\n",
        "# 池化层\n",
        "def max_pool_2x2(x):\n",
        "  # ksize 窗口的大小，第 0 3 位置设置成 1，中间两个数代表池化窗口大小 2*2\n",
        "  # \n",
        "  return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
        "\n",
        "x = tf.placeholder(tf.float32,[None,784])   \n",
        "y = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "# 改变 x 的格式转为 4D 的向量 [batch,in_height,in_width,in_channels]\n",
        "x_image = tf.reshape(x,[-1,28,28,1])\n",
        "\n",
        "# 初始化第一个卷积层的权值和偏重\n",
        "W_conv1 = weight_variable([5,5,1,32])  # 5*5 的采样窗口，32 个卷积核从一个平面抽取特征\n",
        "b_conv1 = bias_variable([32])  # 每一个卷积核一个偏置项\n",
        "\n",
        "# 把 x_image 和权值向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1) # 进行 max-pooling \n",
        "\n",
        "# 初始化第二个卷积层的权值和偏重\n",
        "W_conv2 = weight_variable([5,5,32,64])  # 5*5 的采样窗口，64 个卷积核从 32 个平面抽取特征\n",
        "b_conv2 = bias_variable([64])  # 每一个卷积核一个偏置项\n",
        "\n",
        "# 把 x_image 和权值向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2) # 进行 max-pooling \n",
        "\n",
        "\n",
        "# 28*28 的图片第一次卷积后还是 28*28，第一次池化后变为 14*14，第二次卷积后为 14*14，第二次\n",
        "# 池化后变为 7*7，经过上面操作后得到 64 张 7*7 的平面\n",
        "\n",
        "# 初始化第一个全连接层的权值\n",
        "W_fc1 = weight_variable([7*7*64,1024])  # 上一层有 7*7*64 个神经元，全连接层有 1024 个神经元\n",
        "b_fc1 = bias_variable([1024])  # 1024 个节点\n",
        "\n",
        "# 把池化层2的输出扁平化为 1 维\n",
        "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
        "# 求第一个全连接层的输出\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
        "\n",
        "# keep_prob 用来表示神经元的输出概率\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
        "\n",
        "# 初始化第二个全连接层\n",
        "W_fc2 = weight_variable([1024,10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "# 计算输出\n",
        "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n",
        "\n",
        "# 交叉熵代价函数\n",
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))  # 交叉熵损失\n",
        "\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 结果存放在一个布尔型变量中\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))  # argmax 返回一维张量中最大的值所在的位置\n",
        "\n",
        "# 求准确率\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))  # 转换类型，由布尔型到 float32\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(5):  # 训练 21 次\n",
        "        for batch in range(n_batch):\n",
        "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)  # 每 100 张依次进行训练\n",
        "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
        "        \n",
        "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})  # 每一次的准确率变化\n",
        "        print('Iter ' + str(epoch) + ', Testing Accuracy ' + str(acc))\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-07362ac80f4b>:66: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-7-07362ac80f4b>:76: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Iter 0, Testing Accuracy 0.9527\n",
            "Iter 1, Testing Accuracy 0.9711\n",
            "Iter 2, Testing Accuracy 0.9786\n",
            "Iter 3, Testing Accuracy 0.9819\n",
            "Iter 4, Testing Accuracy 0.9842\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}