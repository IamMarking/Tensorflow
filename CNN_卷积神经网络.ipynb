{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN 卷积神经网络.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IamMarking/Tensorflow/blob/master/CNN_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-B14e9Of44D",
        "colab_type": "text"
      },
      "source": [
        "#### 传统神经网络存在的问题：\n",
        "- 权值太多，计算量太大，计算起来非常慢\n",
        "- 权值太多，需要大量样本进行计算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGJHwjr3BSo",
        "colab_type": "text"
      },
      "source": [
        "#### 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADnWceyoePBG",
        "colab_type": "code",
        "outputId": "f6ef9268-407c-4776-b683-4cf293f426ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "old_v = tf.logging.get_verbosity()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "tf.logging.set_verbosity(old_v)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMTVeGPn9yxV",
        "colab_type": "code",
        "outputId": "eff40a60-9887-4899-9c48-80a896892fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "batch_size = 100  # 1 次放入 100 张图片进行训练，每张图片维度是 784 维\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size  # 计算一共有多少个批次\n",
        "\n",
        "# 初始化权值\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape,stddev=0.1)  # 生成一个截断的正态分布\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 初始化偏置\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1,shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 卷积层\n",
        "def conv2d(x,W):\n",
        "  # x 输入的 tensor 的维度 [batch,in_height,in_width,in_channels]\n",
        "  # 卷积核的 shape [filter_height,filter_width,in_channels,out_channels]\n",
        "  # 步长第 0 个值和第 3 个值都是 1 ，1 代表 x 方向的步长，2 代表 y 方向的步长\n",
        "  # padding SAME 补零，VALID 不用补零\n",
        "  return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='VALID')  # 2 维的卷积操作，\n",
        "\n",
        "# 池化层\n",
        "def max_pool_2x2(x):\n",
        "  # ksize 窗口的大小，第 1 3 位置设置成 1，中间两个数代表池化窗口大小 2*2\n",
        "  # \n",
        "  return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
        "\n",
        "x = tf.placeholder(tf.float32,[None,784])   \n",
        "y = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "# 改变 x 的格式转为 4D 的向量 [batch,in_height,in_width,in_channels]\n",
        "x_image = tf.reshape(x,[-1,28,28,1])\n",
        "\n",
        "# 初始化第一个卷积层的权值和偏重\n",
        "W_conv1 = weight_variable([5,5,1,32])  # 5*5 的采样窗口，32 个卷积核从一个平面抽取特征\n",
        "b_conv1 = bias_variable([32])  # 每一个卷积核一个偏置项\n",
        "\n",
        "# 把 x_image 和权值向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1) # 进行 max-pooling \n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    batch_xs,batch_ys = mnist.train.next_batch(batch_size)  # 每 100 张依次进行训练\n",
        "    sess.run(h_conv1,feed_dict={x:batch_xs,y:batch_ys})\n",
        "    print(x_image.shape)\n",
        "    print(h_conv1.shape)\n",
        "            "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 28, 28, 1)\n",
            "(?, 24, 24, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8rs6bgbhPli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100  # 1 次放入 100 张图片进行训练，每张图片维度是 784 维\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size  # 计算一共有多少个批次\n",
        "\n",
        "# 初始化权值\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape,stddev=0.1)  # 生成一个截断的正态分布\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 初始化偏置\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1,shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "# 卷积层\n",
        "def conv2d(x,W):\n",
        "  # x 输入的 tensor 的维度 [batch,in_height,in_width,in_channels]\n",
        "  # 卷积核的 shape [filter_height,filter_width,in_channels,out_channels]\n",
        "  # 步长第 0 个值和第 3 个值都是 1 ，1 代表 x 方向的步长，2 代表 y 方向的步长\n",
        "  # padding SAME 补零，VALID 不用补零\n",
        "  return tf.nn.conv1d2d(x,W,strides=[1,1,1,1],padding='SAME')  # 2 维的卷积操作，\n",
        "\n",
        "# 池化层\n",
        "def max_pool_2x2(x):\n",
        "  # ksize 窗口的大小，第 1 3 位置设置成 1，中间两个数代表池化窗口大小 2*2\n",
        "  # \n",
        "  return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
        "\n",
        "x = tf.placeholder(tf.float32,[None,784])   \n",
        "y = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "# 改变 x 的格式转为 4D 的向量 [batch,in_height,in_width,in_channels]\n",
        "x_image = tf.reshape(x,[-1,28,28,1])\n",
        "\n",
        "# 初始化第一个卷积层的权值和偏重\n",
        "W_conv1 = weight_variable([5,5,1,32])  # 5*5 的采样窗口，32 个卷积核从一个平面抽取特征\n",
        "b_conv1 = bias_variable([32])  # 每一个卷积核一个偏置项\n",
        "\n",
        "# 把 x_image 和权值向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1) # 进行 max-pooling \n",
        "\n",
        "# 初始化第二个卷积层的权值和偏重\n",
        "W_conv2 = weight_variable([5,5,32,64])  # 5*5 的采样窗口，64 个卷积核从 32 个平面抽取特征\n",
        "b_conv2 = bias_variable([64])  # 每一个卷积核一个偏置项\n",
        "\n",
        "# 把 x_image 和权值向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2) # 进行 max-pooling \n",
        "\n",
        "\n",
        "# 28*28 的图片第一次卷积后还是 28*28，第一次池化后变为 14*14，第二次卷积后为 14*14，第二次\n",
        "# 池化后变为 7*7，经过上面操作后得到 64 张 7*7 的平面\n",
        "\n",
        "# 初始化第一个全连接层的权值\n",
        "W_fc1 = weight_variable([7*7*64,1024])  # 上一层有 7*7*64 个神经元，全连接层有 1024 个神经元\n",
        "b_fc1 = bias_variable([1024])  # 1024 个节点\n",
        "\n",
        "# 把池化层2的输出扁平化为 1 维\n",
        "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
        "# 求第一个全连接层的输出\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)\n",
        "\n",
        "# keep_prob 用来表示神经元的输出概率\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}